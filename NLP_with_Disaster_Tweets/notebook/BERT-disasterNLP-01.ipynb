{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Exploration Series\n",
    "\n",
    "### Twitter Disaster Analysis\n",
    "### Version 01\n",
    "Kaggle Link: https://www.kaggle.com/c/nlp-getting-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Use Huggingface's transformers library\n",
    "- Convert text using BERT tokenizer line by line\n",
    "- Add dense layer to the end of pooling layer\n",
    "- Use masking and padding\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 18 03:08:09 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = Path.cwd()\n",
    "DATA_DIR = CURRENT_DIR.parent / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train set and test set\n",
    "test = pd.read_csv(str(DATA_DIR / 'test.csv'))\n",
    "train = pd.read_csv(str(DATA_DIR / 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4957ff58fd149519172b6f926ef693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Model, self).__init__()\n",
    "        # pre-trained BERT model by HuggingFace\n",
    "        self.base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # add dense layer to the end\n",
    "        self.fc1 = torch.nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, ids, masks):\n",
    "        x = self.base_model(ids, attention_mask=masks)[1]\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch define device\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e4839c75a74dc9a55d88ee586c4f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e16d78f4bb4887a916baa746e8f0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (base_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and put on GPU\n",
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(text, tokenizer, max_len=160):\n",
    "    \"\"\" BERT encoder for text \n",
    "    \n",
    "    Return:\n",
    "    @tokens: input token ids with 0s padding\n",
    "    @pad_masks: 1 for inputs and 0 for paddings\n",
    "    \"\"\"\n",
    "    # tokenize text using BERT tokenizer\n",
    "    text = tokenizer.tokenize(text)\n",
    "    # remove 2 tokens for start and end token\n",
    "    text = text[:max_len-2]\n",
    "    # add start and end token\n",
    "    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "    # convert token to token_id\n",
    "    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "    # the rest of max_len need to be pad\n",
    "    pad_len = max_len - len(input_sequence)\n",
    "    # padding to max_len\n",
    "    tokens += [0] * pad_len\n",
    "    # masking, 1 for inputs and 0 for paddings\n",
    "    pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "    return tokens, pad_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Basic text cleaning \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9] \\n', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first 6000 for training, rest for validation\n",
    "train_text = train.text[:6000]\n",
    "val_text = train.text[6000:]\n",
    "# clean text\n",
    "train_text = train_text.apply(clean_text)\n",
    "val_text = val_text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text and get mask\n",
    "train_tokens = []\n",
    "train_pad_masks = []\n",
    "for text in train_text:\n",
    "    tokens, masks = bert_encode(text)\n",
    "    train_tokens.append(tokens)\n",
    "    train_pad_masks.append(masks)\n",
    "    \n",
    "train_tokens = np.array(train_tokens)\n",
    "train_pad_masks = np.array(train_pad_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for validation\n",
    "val_tokens = []\n",
    "val_pad_masks = []\n",
    "for text in val_text:\n",
    "    tokens, masks = bert_encode(text)\n",
    "    val_tokens.append(tokens)\n",
    "    val_pad_masks.append(masks)\n",
    "    \n",
    "val_tokens = np.array(val_tokens)\n",
    "val_pad_masks = np.array(val_pad_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pytorch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, train_tokens, train_pad_masks, targets):\n",
    "        \n",
    "        super(Dataset, self).__init__()\n",
    "        self.train_tokens = train_tokens\n",
    "        self.train_pad_masks = train_pad_masks\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        tokens = self.train_tokens[index]\n",
    "        masks = self.train_pad_masks[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        return (tokens, masks), target\n",
    "    \n",
    "    def __len__(self,):\n",
    "        \n",
    "        return len(self.train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataset\n",
    "train_dataset = Dataset(\n",
    "                    train_tokens=train_tokens,\n",
    "                    train_pad_masks=train_pad_masks,\n",
    "                    targets=train.target[:6000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "batch_size = 12\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# Use Adam Optimizer with learning rate of 0.00001\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Step: 1\n",
      "Step: 2\n",
      "Step: 3\n",
      "Step: 4\n",
      "Step: 5\n",
      "Step: 6\n",
      "Step: 7\n",
      "Step: 8\n",
      "Step: 9\n",
      "Step: 10\n",
      "Step: 11\n",
      "Step: 12\n",
      "Step: 13\n",
      "Step: 14\n",
      "Step: 15\n",
      "Step: 16\n",
      "Step: 17\n",
      "Step: 18\n",
      "Step: 19\n",
      "Step: 20\n",
      "Step: 21\n",
      "Step: 22\n",
      "Step: 23\n",
      "Step: 24\n",
      "Step: 25\n",
      "Step: 26\n",
      "Step: 27\n",
      "Step: 28\n",
      "Step: 29\n",
      "Step: 30\n",
      "Step: 31\n",
      "Step: 32\n",
      "Step: 33\n",
      "Step: 34\n",
      "Step: 35\n",
      "Step: 36\n",
      "Step: 37\n",
      "Step: 38\n",
      "Step: 39\n",
      "Step: 40\n",
      "Step: 41\n",
      "Step: 42\n",
      "Step: 43\n",
      "Step: 44\n",
      "Step: 45\n",
      "Step: 46\n",
      "Step: 47\n",
      "Step: 48\n",
      "Step: 49\n",
      "Step: 50\n",
      "Step: 51\n",
      "Step: 52\n",
      "Step: 53\n",
      "Step: 54\n",
      "Step: 55\n",
      "Step: 56\n",
      "Step: 57\n",
      "Step: 58\n",
      "Step: 59\n",
      "Step: 60\n",
      "Step: 61\n",
      "Step: 62\n",
      "Step: 63\n",
      "Step: 64\n",
      "Step: 65\n",
      "Step: 66\n",
      "Step: 67\n",
      "Step: 68\n",
      "Step: 69\n",
      "Step: 70\n",
      "Step: 71\n",
      "Step: 72\n",
      "Step: 73\n",
      "Step: 74\n",
      "Step: 75\n",
      "Step: 76\n",
      "Step: 77\n",
      "Step: 78\n",
      "Step: 79\n",
      "Step: 80\n",
      "Step: 81\n",
      "Step: 82\n",
      "Step: 83\n",
      "Step: 84\n",
      "Step: 85\n",
      "Step: 86\n",
      "Step: 87\n",
      "Step: 88\n",
      "Step: 89\n",
      "Step: 90\n",
      "Step: 91\n",
      "Step: 92\n",
      "Step: 93\n",
      "Step: 94\n",
      "Step: 95\n",
      "Step: 96\n",
      "Step: 97\n",
      "Step: 98\n",
      "Step: 99\n",
      "Step: 100\n",
      "Step: 101\n",
      "Step: 102\n",
      "Step: 103\n",
      "Step: 104\n",
      "Step: 105\n",
      "Step: 106\n",
      "Step: 107\n",
      "Step: 108\n",
      "Step: 109\n",
      "Step: 110\n",
      "Step: 111\n",
      "Step: 112\n",
      "Step: 113\n",
      "Step: 114\n",
      "Step: 115\n",
      "Step: 116\n",
      "Step: 117\n",
      "Step: 118\n",
      "Step: 119\n",
      "Step: 120\n",
      "Step: 121\n",
      "Step: 122\n",
      "Step: 123\n",
      "Step: 124\n",
      "Step: 125\n",
      "Step: 126\n",
      "Step: 127\n",
      "Step: 128\n",
      "Step: 129\n",
      "Step: 130\n",
      "Step: 131\n",
      "Step: 132\n",
      "Step: 133\n",
      "Step: 134\n",
      "Step: 135\n",
      "Step: 136\n",
      "Step: 137\n",
      "Step: 138\n",
      "Step: 139\n",
      "Step: 140\n",
      "Step: 141\n",
      "Step: 142\n",
      "Step: 143\n",
      "Step: 144\n",
      "Step: 145\n",
      "Step: 146\n",
      "Step: 147\n",
      "Step: 148\n",
      "Step: 149\n",
      "Step: 150\n",
      "Step: 151\n",
      "Step: 152\n",
      "Step: 153\n",
      "Step: 154\n",
      "Step: 155\n",
      "Step: 156\n",
      "Step: 157\n",
      "Step: 158\n",
      "Step: 159\n",
      "Step: 160\n",
      "Step: 161\n",
      "Step: 162\n",
      "Step: 163\n",
      "Step: 164\n",
      "Step: 165\n",
      "Step: 166\n",
      "Step: 167\n",
      "Step: 168\n",
      "Step: 169\n",
      "Step: 170\n",
      "Step: 171\n",
      "Step: 172\n",
      "Step: 173\n",
      "Step: 174\n",
      "Step: 175\n",
      "Step: 176\n",
      "Step: 177\n",
      "Step: 178\n",
      "Step: 179\n",
      "Step: 180\n",
      "Step: 181\n",
      "Step: 182\n",
      "Step: 183\n",
      "Step: 184\n",
      "Step: 185\n",
      "Step: 186\n",
      "Step: 187\n",
      "Step: 188\n",
      "Step: 189\n",
      "Step: 190\n",
      "Step: 191\n",
      "Step: 192\n",
      "Step: 193\n",
      "Step: 194\n",
      "Step: 195\n",
      "Step: 196\n",
      "Step: 197\n",
      "Step: 198\n",
      "Step: 199\n",
      "Step: 200\n",
      "Step: 201\n",
      "Step: 202\n",
      "Step: 203\n",
      "Step: 204\n",
      "Step: 205\n",
      "Step: 206\n",
      "Step: 207\n",
      "Step: 208\n",
      "Step: 209\n",
      "Step: 210\n",
      "Step: 211\n",
      "Step: 212\n",
      "Step: 213\n",
      "Step: 214\n",
      "Step: 215\n",
      "Step: 216\n",
      "Step: 217\n",
      "Step: 218\n",
      "Step: 219\n",
      "Step: 220\n",
      "Step: 221\n",
      "Step: 222\n",
      "Step: 223\n",
      "Step: 224\n",
      "Step: 225\n",
      "Step: 226\n",
      "Step: 227\n",
      "Step: 228\n",
      "Step: 229\n",
      "Step: 230\n",
      "Step: 231\n",
      "Step: 232\n",
      "Step: 233\n",
      "Step: 234\n",
      "Step: 235\n",
      "Step: 236\n",
      "Step: 237\n",
      "Step: 238\n",
      "Step: 239\n",
      "Step: 240\n",
      "Step: 241\n",
      "Step: 242\n",
      "Step: 243\n",
      "Step: 244\n",
      "Step: 245\n",
      "Step: 246\n",
      "Step: 247\n",
      "Step: 248\n",
      "Step: 249\n",
      "Step: 250\n",
      "Step: 251\n",
      "Step: 252\n",
      "Step: 253\n",
      "Step: 254\n",
      "Step: 255\n",
      "Step: 256\n",
      "Step: 257\n",
      "Step: 258\n",
      "Step: 259\n",
      "Step: 260\n",
      "Step: 261\n",
      "Step: 262\n",
      "Step: 263\n",
      "Step: 264\n",
      "Step: 265\n",
      "Step: 266\n",
      "Step: 267\n",
      "Step: 268\n",
      "Step: 269\n",
      "Step: 270\n",
      "Step: 271\n",
      "Step: 272\n",
      "Step: 273\n",
      "Step: 274\n",
      "Step: 275\n",
      "Step: 276\n",
      "Step: 277\n",
      "Step: 278\n",
      "Step: 279\n",
      "Step: 280\n",
      "Step: 281\n",
      "Step: 282\n",
      "Step: 283\n",
      "Step: 284\n",
      "Step: 285\n",
      "Step: 286\n",
      "Step: 287\n",
      "Step: 288\n",
      "Step: 289\n",
      "Step: 290\n",
      "Step: 291\n",
      "Step: 292\n",
      "Step: 293\n",
      "Step: 294\n",
      "Step: 295\n",
      "Step: 296\n",
      "Step: 297\n",
      "Step: 298\n",
      "Step: 299\n",
      "Step: 300\n",
      "Step: 301\n",
      "Step: 302\n",
      "Step: 303\n",
      "Step: 304\n",
      "Step: 305\n",
      "Step: 306\n",
      "Step: 307\n",
      "Step: 308\n",
      "Step: 309\n",
      "Step: 310\n",
      "Step: 311\n",
      "Step: 312\n",
      "Step: 313\n",
      "Step: 314\n",
      "Step: 315\n",
      "Step: 316\n",
      "Step: 317\n",
      "Step: 318\n",
      "Step: 319\n",
      "Step: 320\n",
      "Step: 321\n",
      "Step: 322\n",
      "Step: 323\n",
      "Step: 324\n",
      "Step: 325\n",
      "Step: 326\n",
      "Step: 327\n",
      "Step: 328\n",
      "Step: 329\n",
      "Step: 330\n",
      "Step: 331\n",
      "Step: 332\n",
      "Step: 333\n",
      "Step: 334\n",
      "Step: 335\n",
      "Step: 336\n",
      "Step: 337\n",
      "Step: 338\n",
      "Step: 339\n",
      "Step: 340\n",
      "Step: 341\n",
      "Step: 342\n",
      "Step: 343\n",
      "Step: 344\n",
      "Step: 345\n",
      "Step: 346\n",
      "Step: 347\n",
      "Step: 348\n",
      "Step: 349\n",
      "Step: 350\n",
      "Step: 351\n",
      "Step: 352\n",
      "Step: 353\n",
      "Step: 354\n",
      "Step: 355\n",
      "Step: 356\n",
      "Step: 357\n",
      "Step: 358\n",
      "Step: 359\n",
      "Step: 360\n",
      "Step: 361\n",
      "Step: 362\n",
      "Step: 363\n",
      "Step: 364\n",
      "Step: 365\n",
      "Step: 366\n",
      "Step: 367\n",
      "Step: 368\n",
      "Step: 369\n",
      "Step: 370\n",
      "Step: 371\n",
      "Step: 372\n",
      "Step: 373\n",
      "Step: 374\n",
      "Step: 375\n",
      "Step: 376\n",
      "Step: 377\n",
      "Step: 378\n",
      "Step: 379\n",
      "Step: 380\n",
      "Step: 381\n",
      "Step: 382\n",
      "Step: 383\n",
      "Step: 384\n",
      "Step: 385\n",
      "Step: 386\n",
      "Step: 387\n",
      "Step: 388\n",
      "Step: 389\n",
      "Step: 390\n",
      "Step: 391\n",
      "Step: 392\n",
      "Step: 393\n",
      "Step: 394\n",
      "Step: 395\n",
      "Step: 396\n",
      "Step: 397\n",
      "Step: 398\n",
      "Step: 399\n",
      "Step: 400\n",
      "Step: 401\n",
      "Step: 402\n",
      "Step: 403\n",
      "Step: 404\n",
      "Step: 405\n",
      "Step: 406\n",
      "Step: 407\n",
      "Step: 408\n",
      "Step: 409\n",
      "Step: 410\n",
      "Step: 411\n",
      "Step: 412\n",
      "Step: 413\n",
      "Step: 414\n",
      "Step: 415\n",
      "Step: 416\n",
      "Step: 417\n",
      "Step: 418\n",
      "Step: 419\n",
      "Step: 420\n",
      "Step: 421\n",
      "Step: 422\n",
      "Step: 423\n",
      "Step: 424\n",
      "Step: 425\n",
      "Step: 426\n",
      "Step: 427\n",
      "Step: 428\n",
      "Step: 429\n",
      "Step: 430\n",
      "Step: 431\n",
      "Step: 432\n",
      "Step: 433\n",
      "Step: 434\n",
      "Step: 435\n",
      "Step: 436\n",
      "Step: 437\n",
      "Step: 438\n",
      "Step: 439\n",
      "Step: 440\n",
      "Step: 441\n",
      "Step: 442\n",
      "Step: 443\n",
      "Step: 444\n",
      "Step: 445\n",
      "Step: 446\n",
      "Step: 447\n",
      "Step: 448\n",
      "Step: 449\n",
      "Step: 450\n",
      "Step: 451\n",
      "Step: 452\n",
      "Step: 453\n",
      "Step: 454\n",
      "Step: 455\n",
      "Step: 456\n",
      "Step: 457\n",
      "Step: 458\n",
      "Step: 459\n",
      "Step: 460\n",
      "Step: 461\n",
      "Step: 462\n",
      "Step: 463\n",
      "Step: 464\n",
      "Step: 465\n",
      "Step: 466\n",
      "Step: 467\n",
      "Step: 468\n",
      "Step: 469\n",
      "Step: 470\n",
      "Step: 471\n",
      "Step: 472\n",
      "Step: 473\n",
      "Step: 474\n",
      "Step: 475\n",
      "Step: 476\n",
      "Step: 477\n",
      "Step: 478\n",
      "Step: 479\n",
      "Step: 480\n",
      "Step: 481\n",
      "Step: 482\n",
      "Step: 483\n",
      "Step: 484\n",
      "Step: 485\n",
      "Step: 486\n",
      "Step: 487\n",
      "Step: 488\n",
      "Step: 489\n",
      "Step: 490\n",
      "Step: 491\n",
      "Step: 492\n",
      "Step: 493\n",
      "Step: 494\n",
      "Step: 495\n",
      "Step: 496\n",
      "Step: 497\n",
      "Step: 498\n",
      "Step: 499\n",
      "Epoch: 1/2, 100.000000% loss: 0.30\n",
      "Step: 0\n",
      "Step: 1\n",
      "Step: 2\n",
      "Step: 3\n",
      "Step: 4\n",
      "Step: 5\n",
      "Step: 6\n",
      "Step: 7\n",
      "Step: 8\n",
      "Step: 9\n",
      "Step: 10\n",
      "Step: 11\n",
      "Step: 12\n",
      "Step: 13\n",
      "Step: 14\n",
      "Step: 15\n",
      "Step: 16\n",
      "Step: 17\n",
      "Step: 18\n",
      "Step: 19\n",
      "Step: 20\n",
      "Step: 21\n",
      "Step: 22\n",
      "Step: 23\n",
      "Step: 24\n",
      "Step: 25\n",
      "Step: 26\n",
      "Step: 27\n",
      "Step: 28\n",
      "Step: 29\n",
      "Step: 30\n",
      "Step: 31\n",
      "Step: 32\n",
      "Step: 33\n",
      "Step: 34\n",
      "Step: 35\n",
      "Step: 36\n",
      "Step: 37\n",
      "Step: 38\n",
      "Step: 39\n",
      "Step: 40\n",
      "Step: 41\n",
      "Step: 42\n",
      "Step: 43\n",
      "Step: 44\n",
      "Step: 45\n",
      "Step: 46\n",
      "Step: 47\n",
      "Step: 48\n",
      "Step: 49\n",
      "Step: 50\n",
      "Step: 51\n",
      "Step: 52\n",
      "Step: 53\n",
      "Step: 54\n",
      "Step: 55\n",
      "Step: 56\n",
      "Step: 57\n",
      "Step: 58\n",
      "Step: 59\n",
      "Step: 60\n",
      "Step: 61\n",
      "Step: 62\n",
      "Step: 63\n",
      "Step: 64\n",
      "Step: 65\n",
      "Step: 66\n",
      "Step: 67\n",
      "Step: 68\n",
      "Step: 69\n",
      "Step: 70\n",
      "Step: 71\n",
      "Step: 72\n",
      "Step: 73\n",
      "Step: 74\n",
      "Step: 75\n",
      "Step: 76\n",
      "Step: 77\n",
      "Step: 78\n",
      "Step: 79\n",
      "Step: 80\n",
      "Step: 81\n",
      "Step: 82\n",
      "Step: 83\n",
      "Step: 84\n",
      "Step: 85\n",
      "Step: 86\n",
      "Step: 87\n",
      "Step: 88\n",
      "Step: 89\n",
      "Step: 90\n",
      "Step: 91\n",
      "Step: 92\n",
      "Step: 93\n",
      "Step: 94\n",
      "Step: 95\n",
      "Step: 96\n",
      "Step: 97\n",
      "Step: 98\n",
      "Step: 99\n",
      "Step: 100\n",
      "Step: 101\n",
      "Step: 102\n",
      "Step: 103\n",
      "Step: 104\n",
      "Step: 105\n",
      "Step: 106\n",
      "Step: 107\n",
      "Step: 108\n",
      "Step: 109\n",
      "Step: 110\n",
      "Step: 111\n",
      "Step: 112\n",
      "Step: 113\n",
      "Step: 114\n",
      "Step: 115\n",
      "Step: 116\n",
      "Step: 117\n",
      "Step: 118\n",
      "Step: 119\n",
      "Step: 120\n",
      "Step: 121\n",
      "Step: 122\n",
      "Step: 123\n",
      "Step: 124\n",
      "Step: 125\n",
      "Step: 126\n",
      "Step: 127\n",
      "Step: 128\n",
      "Step: 129\n",
      "Step: 130\n",
      "Step: 131\n",
      "Step: 132\n",
      "Step: 133\n",
      "Step: 134\n",
      "Step: 135\n",
      "Step: 136\n",
      "Step: 137\n",
      "Step: 138\n",
      "Step: 139\n",
      "Step: 140\n",
      "Step: 141\n",
      "Step: 142\n",
      "Step: 143\n",
      "Step: 144\n",
      "Step: 145\n",
      "Step: 146\n",
      "Step: 147\n",
      "Step: 148\n",
      "Step: 149\n",
      "Step: 150\n",
      "Step: 151\n",
      "Step: 152\n",
      "Step: 153\n",
      "Step: 154\n",
      "Step: 155\n",
      "Step: 156\n",
      "Step: 157\n",
      "Step: 158\n",
      "Step: 159\n",
      "Step: 160\n",
      "Step: 161\n",
      "Step: 162\n",
      "Step: 163\n",
      "Step: 164\n",
      "Step: 165\n",
      "Step: 166\n",
      "Step: 167\n",
      "Step: 168\n",
      "Step: 169\n",
      "Step: 170\n",
      "Step: 171\n",
      "Step: 172\n",
      "Step: 173\n",
      "Step: 174\n",
      "Step: 175\n",
      "Step: 176\n",
      "Step: 177\n",
      "Step: 178\n",
      "Step: 179\n",
      "Step: 180\n",
      "Step: 181\n",
      "Step: 182\n",
      "Step: 183\n",
      "Step: 184\n",
      "Step: 185\n",
      "Step: 186\n",
      "Step: 187\n",
      "Step: 188\n",
      "Step: 189\n",
      "Step: 190\n",
      "Step: 191\n",
      "Step: 192\n",
      "Step: 193\n",
      "Step: 194\n",
      "Step: 195\n",
      "Step: 196\n",
      "Step: 197\n",
      "Step: 198\n",
      "Step: 199\n",
      "Step: 200\n",
      "Step: 201\n",
      "Step: 202\n",
      "Step: 203\n",
      "Step: 204\n",
      "Step: 205\n",
      "Step: 206\n",
      "Step: 207\n",
      "Step: 208\n",
      "Step: 209\n",
      "Step: 210\n",
      "Step: 211\n",
      "Step: 212\n",
      "Step: 213\n",
      "Step: 214\n",
      "Step: 215\n",
      "Step: 216\n",
      "Step: 217\n",
      "Step: 218\n",
      "Step: 219\n",
      "Step: 220\n",
      "Step: 221\n",
      "Step: 222\n",
      "Step: 223\n",
      "Step: 224\n",
      "Step: 225\n",
      "Step: 226\n",
      "Step: 227\n",
      "Step: 228\n",
      "Step: 229\n",
      "Step: 230\n",
      "Step: 231\n",
      "Step: 232\n",
      "Step: 233\n",
      "Step: 234\n",
      "Step: 235\n",
      "Step: 236\n",
      "Step: 237\n",
      "Step: 238\n",
      "Step: 239\n",
      "Step: 240\n",
      "Step: 241\n",
      "Step: 242\n",
      "Step: 243\n",
      "Step: 244\n",
      "Step: 245\n",
      "Step: 246\n",
      "Step: 247\n",
      "Step: 248\n",
      "Step: 249\n",
      "Step: 250\n",
      "Step: 251\n",
      "Step: 252\n",
      "Step: 253\n",
      "Step: 254\n",
      "Step: 255\n",
      "Step: 256\n",
      "Step: 257\n",
      "Step: 258\n",
      "Step: 259\n",
      "Step: 260\n",
      "Step: 261\n",
      "Step: 262\n",
      "Step: 263\n",
      "Step: 264\n",
      "Step: 265\n",
      "Step: 266\n",
      "Step: 267\n",
      "Step: 268\n",
      "Step: 269\n",
      "Step: 270\n",
      "Step: 271\n",
      "Step: 272\n",
      "Step: 273\n",
      "Step: 274\n",
      "Step: 275\n",
      "Step: 276\n",
      "Step: 277\n",
      "Step: 278\n",
      "Step: 279\n",
      "Step: 280\n",
      "Step: 281\n",
      "Step: 282\n",
      "Step: 283\n",
      "Step: 284\n",
      "Step: 285\n",
      "Step: 286\n",
      "Step: 287\n",
      "Step: 288\n",
      "Step: 289\n",
      "Step: 290\n",
      "Step: 291\n",
      "Step: 292\n",
      "Step: 293\n",
      "Step: 294\n",
      "Step: 295\n",
      "Step: 296\n",
      "Step: 297\n",
      "Step: 298\n",
      "Step: 299\n",
      "Step: 300\n",
      "Step: 301\n",
      "Step: 302\n",
      "Step: 303\n",
      "Step: 304\n",
      "Step: 305\n",
      "Step: 306\n",
      "Step: 307\n",
      "Step: 308\n",
      "Step: 309\n",
      "Step: 310\n",
      "Step: 311\n",
      "Step: 312\n",
      "Step: 313\n",
      "Step: 314\n",
      "Step: 315\n",
      "Step: 316\n",
      "Step: 317\n",
      "Step: 318\n",
      "Step: 319\n",
      "Step: 320\n",
      "Step: 321\n",
      "Step: 322\n",
      "Step: 323\n",
      "Step: 324\n",
      "Step: 325\n",
      "Step: 326\n",
      "Step: 327\n",
      "Step: 328\n",
      "Step: 329\n",
      "Step: 330\n",
      "Step: 331\n",
      "Step: 332\n",
      "Step: 333\n",
      "Step: 334\n",
      "Step: 335\n",
      "Step: 336\n",
      "Step: 337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 338\n",
      "Step: 339\n",
      "Step: 340\n",
      "Step: 341\n",
      "Step: 342\n",
      "Step: 343\n",
      "Step: 344\n",
      "Step: 345\n",
      "Step: 346\n",
      "Step: 347\n",
      "Step: 348\n",
      "Step: 349\n",
      "Step: 350\n",
      "Step: 351\n",
      "Step: 352\n",
      "Step: 353\n",
      "Step: 354\n",
      "Step: 355\n",
      "Step: 356\n",
      "Step: 357\n",
      "Step: 358\n",
      "Step: 359\n",
      "Step: 360\n",
      "Step: 361\n",
      "Step: 362\n",
      "Step: 363\n",
      "Step: 364\n",
      "Step: 365\n",
      "Step: 366\n",
      "Step: 367\n",
      "Step: 368\n",
      "Step: 369\n",
      "Step: 370\n",
      "Step: 371\n",
      "Step: 372\n",
      "Step: 373\n",
      "Step: 374\n",
      "Step: 375\n",
      "Step: 376\n",
      "Step: 377\n",
      "Step: 378\n",
      "Step: 379\n",
      "Step: 380\n",
      "Step: 381\n",
      "Step: 382\n",
      "Step: 383\n",
      "Step: 384\n",
      "Step: 385\n",
      "Step: 386\n",
      "Step: 387\n",
      "Step: 388\n",
      "Step: 389\n",
      "Step: 390\n",
      "Step: 391\n",
      "Step: 392\n",
      "Step: 393\n",
      "Step: 394\n",
      "Step: 395\n",
      "Step: 396\n",
      "Step: 397\n",
      "Step: 398\n",
      "Step: 399\n",
      "Step: 400\n",
      "Step: 401\n",
      "Step: 402\n",
      "Step: 403\n",
      "Step: 404\n",
      "Step: 405\n",
      "Step: 406\n",
      "Step: 407\n",
      "Step: 408\n",
      "Step: 409\n",
      "Step: 410\n",
      "Step: 411\n",
      "Step: 412\n",
      "Step: 413\n",
      "Step: 414\n",
      "Step: 415\n",
      "Step: 416\n",
      "Step: 417\n",
      "Step: 418\n",
      "Step: 419\n",
      "Step: 420\n",
      "Step: 421\n",
      "Step: 422\n",
      "Step: 423\n",
      "Step: 424\n",
      "Step: 425\n",
      "Step: 426\n",
      "Step: 427\n",
      "Step: 428\n",
      "Step: 429\n",
      "Step: 430\n",
      "Step: 431\n",
      "Step: 432\n",
      "Step: 433\n",
      "Step: 434\n",
      "Step: 435\n",
      "Step: 436\n",
      "Step: 437\n",
      "Step: 438\n",
      "Step: 439\n",
      "Step: 440\n",
      "Step: 441\n",
      "Step: 442\n",
      "Step: 443\n",
      "Step: 444\n",
      "Step: 445\n",
      "Step: 446\n",
      "Step: 447\n",
      "Step: 448\n",
      "Step: 449\n",
      "Step: 450\n",
      "Step: 451\n",
      "Step: 452\n",
      "Step: 453\n",
      "Step: 454\n",
      "Step: 455\n",
      "Step: 456\n",
      "Step: 457\n",
      "Step: 458\n",
      "Step: 459\n",
      "Step: 460\n",
      "Step: 461\n",
      "Step: 462\n",
      "Step: 463\n",
      "Step: 464\n",
      "Step: 465\n",
      "Step: 466\n",
      "Step: 467\n",
      "Step: 468\n",
      "Step: 469\n",
      "Step: 470\n",
      "Step: 471\n",
      "Step: 472\n",
      "Step: 473\n",
      "Step: 474\n",
      "Step: 475\n",
      "Step: 476\n",
      "Step: 477\n",
      "Step: 478\n",
      "Step: 479\n",
      "Step: 480\n",
      "Step: 481\n",
      "Step: 482\n",
      "Step: 483\n",
      "Step: 484\n",
      "Step: 485\n",
      "Step: 486\n",
      "Step: 487\n",
      "Step: 488\n",
      "Step: 489\n",
      "Step: 490\n",
      "Step: 491\n",
      "Step: 492\n",
      "Step: 493\n",
      "Step: 494\n",
      "Step: 495\n",
      "Step: 496\n",
      "Step: 497\n",
      "Step: 498\n",
      "Step: 499\n",
      "Epoch: 2/2, 100.000000% loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "y_preds = []\n",
    "\n",
    "# train 2 epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for i, ((tokens, masks), target) in enumerate(train_dataloader):\n",
    "\n",
    "        y_pred = model(\n",
    "                    tokens.long().to(device), \n",
    "                    masks.long().to(device)\n",
    "                )\n",
    "        loss = criterion(y_pred, target[:, None].float().to(device))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # print(\"Step:\", i)\n",
    "    print('\\rEpoch: %d/%d, %f%% loss: %0.2f'% (epoch+1, EPOCHS, (i+1)/len(train_dataloader)*100, loss.item()), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation dataset and dataloader\n",
    "val_dataset = Dataset(\n",
    "                    train_tokens=val_tokens,\n",
    "                    train_pad_masks=val_pad_masks,\n",
    "                    targets=train.target[6000:].reset_index(drop=True)\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metric\n",
    "def accuracy(y_actual, y_pred):\n",
    "    y_ = y_pred > 0\n",
    "    return np.sum(y_actual == y_).astype('int') / y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.81% loss: 0.02, accuracy 1.00\n",
      "Average accuracy:  0.8203221809169773\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on val dataset\n",
    "model.eval()\n",
    "avg_acc = 0\n",
    "for i, ((tokens, masks), target) in enumerate(val_dataloader):\n",
    "\n",
    "    y_pred = model(\n",
    "                tokens.long().to(device), \n",
    "                masks.long().to(device), \n",
    "            )\n",
    "    loss = criterion(y_pred,  target[:, None].float().to(device))\n",
    "    acc = accuracy(target.cpu().numpy(), y_pred.detach().cpu().numpy().squeeze())\n",
    "    avg_acc += acc\n",
    "    print('\\r%0.2f%% loss: %0.2f, accuracy %0.2f'% (i/len(val_dataloader)*100, loss.item(), acc), end='')\n",
    "print('\\nAverage accuracy: ', avg_acc / len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test dataset\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, test_tokens, test_pad_masks):\n",
    "        \n",
    "        super(TestDataset, self).__init__()\n",
    "        self.test_tokens = test_tokens\n",
    "        self.test_pad_masks = test_pad_masks\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        tokens = self.test_tokens[index]\n",
    "        masks = self.test_pad_masks[index]\n",
    "        \n",
    "        return (tokens, masks)\n",
    "    \n",
    "    def __len__(self,):\n",
    "        \n",
    "        return len(self.test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode test text and get mask\n",
    "test_tokens = []\n",
    "test_pad_masks = []\n",
    "for text in test.text:\n",
    "    tokens, masks = bert_encode(text)\n",
    "    test_tokens.append(tokens)\n",
    "    test_pad_masks.append(masks)\n",
    "    \n",
    "test_tokens = np.array(test_tokens)\n",
    "test_pad_masks = np.array(test_pad_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test dataset and dataloader\n",
    "test_dataset = TestDataset(\n",
    "    test_tokens=test_tokens,\n",
    "    test_pad_masks=test_pad_masks\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get result from test dataset\n",
    "model.eval()\n",
    "y_preds = []\n",
    "for (tokens, masks) in test_dataloader:\n",
    "\n",
    "    y_pred = model(\n",
    "                tokens.long().to(device), \n",
    "                masks.long().to(device), \n",
    "            )\n",
    "    y_preds += y_pred.detach().cpu().numpy().squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1876\n",
      "1    1387\n",
      "Name: target, dtype: int64\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "# get submission dataframe\n",
    "submission_df = pd.read_csv(str(DATA_DIR / 'sample_submission.csv'))\n",
    "submission_df['target'] = (np.array(y_preds) > 0).astype('int')\n",
    "print(submission_df.target.value_counts())\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv\n",
    "submission_df.to_csv(str(DATA_DIR / 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_alert)",
   "language": "python",
   "name": "conda_alert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
